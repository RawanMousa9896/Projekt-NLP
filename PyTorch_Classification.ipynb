{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show torch\n",
    "#!pip show torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torchtext) (4.66.5)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from torchtext) (2.32.3)\n",
      "Requirement already satisfied: torch>=2.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from torchtext) (2.4.1+cpu)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from torchtext) (2.1.1)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (75.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->torchtext) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->torchtext) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->torchtext) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch torchvision scikit-learn\n",
    "#!pip install --upgrade torch torchtext\n",
    "#!pip install nltk\n",
    "#!pip install torch==2.4.1 torchtext==0.18.0\n",
    "#!pip install --upgrade torch torchtext\n",
    "# !pip uninstall torch torchtext\n",
    "#!pip install torchtext==0.18.1\n",
    "!pip install --upgrade torchtext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>release_year</th>\n",
       "      <th>language</th>\n",
       "      <th>genre</th>\n",
       "      <th>overview</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>popularity</th>\n",
       "      <th>cleaned_overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pad Man</td>\n",
       "      <td>2018</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>upon realizing extent woman affected menses se...</td>\n",
       "      <td>7.420</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.036</td>\n",
       "      <td>upon realizing extent woman affected menses se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tamasha</td>\n",
       "      <td>2015</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>meeting vacation ved tara sense connection vow...</td>\n",
       "      <td>6.720</td>\n",
       "      <td>141.0</td>\n",
       "      <td>8.770</td>\n",
       "      <td>meeting vacation ved tara sense connection vow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tu Jhoothi Main Makkaar</td>\n",
       "      <td>2023</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>earn extra cash mickey help couple break life ...</td>\n",
       "      <td>6.253</td>\n",
       "      <td>144.0</td>\n",
       "      <td>10.045</td>\n",
       "      <td>earn extra cash mickey help couple break life ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi Medium</td>\n",
       "      <td>2017</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>mita raj batra affluent couple delhi chandni c...</td>\n",
       "      <td>7.300</td>\n",
       "      <td>166.0</td>\n",
       "      <td>7.001</td>\n",
       "      <td>mita raj batra affluent couple delhi chandni c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dilwale</td>\n",
       "      <td>2015</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>raj mafia member one day meet girl meera chasi...</td>\n",
       "      <td>6.648</td>\n",
       "      <td>301.0</td>\n",
       "      <td>11.501</td>\n",
       "      <td>raj mafia member one day meet girl meera chasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title  release_year language  genre  \\\n",
       "0                  Pad Man          2018  Indisch  Humor   \n",
       "1                  Tamasha          2015  Indisch  Humor   \n",
       "2  Tu Jhoothi Main Makkaar          2023  Indisch  Humor   \n",
       "3             Hindi Medium          2017  Indisch  Humor   \n",
       "4                  Dilwale          2015  Indisch  Humor   \n",
       "\n",
       "                                            overview  vote_average  \\\n",
       "0  upon realizing extent woman affected menses se...         7.420   \n",
       "1  meeting vacation ved tara sense connection vow...         6.720   \n",
       "2  earn extra cash mickey help couple break life ...         6.253   \n",
       "3  mita raj batra affluent couple delhi chandni c...         7.300   \n",
       "4  raj mafia member one day meet girl meera chasi...         6.648   \n",
       "\n",
       "   vote_count  popularity                                   cleaned_overview  \n",
       "0       200.0       7.036  upon realizing extent woman affected menses se...  \n",
       "1       141.0       8.770  meeting vacation ved tara sense connection vow...  \n",
       "2       144.0      10.045  earn extra cash mickey help couple break life ...  \n",
       "3       166.0       7.001  mita raj batra affluent couple delhi chandni c...  \n",
       "4       301.0      11.501  raj mafia member one day meet girl meera chasi...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/movies_2015_2023_preprocessed_genre.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in 'cleaned_overview' with an empty string\n",
    "df['cleaned_overview'] = df['cleaned_overview'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing cleaned_overview\n",
    "df = df.dropna(subset=['cleaned_overview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Convert the 'language' column to numeric labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of value tuples: 15500\n",
      "First tuple: (3, 'upon realizing extent woman affected menses set create sanitary pad machine provide inexpensive sanitary pad woman rural india')\n",
      "Last tuple: (4, 'several european underground director turn ancient rule mankind inside shocking goresoaked interpretation god law bible world ending commandment dying')\n"
     ]
    }
   ],
   "source": [
    "# Create value tuples (label, cleaned_overview)\n",
    "value_tuples = []\n",
    "for _, row in df.iterrows():\n",
    "    value_tuples.append((row['label'], row['cleaned_overview']))\n",
    "\n",
    "# Print the length and a few examples\n",
    "print(f\"Total number of value tuples: {len(value_tuples)}\")\n",
    "print(\"First tuple:\", value_tuples[0])\n",
    "print(\"Last tuple:\", value_tuples[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 12400\n",
      "Testing set size: 3100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 80% training and 20% testing\n",
    "train_value_tuples, test_value_tuples = train_test_split(value_tuples, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the size of the training and testing sets\n",
    "print(f\"Training set size: {len(train_value_tuples)}\")\n",
    "print(f\"Testing set size: {len(test_value_tuples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasic_english\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_ops.py:1295\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1290\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m-> 1295\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/ctypes/__init__.py:379\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /usr/local/python/3.12.1/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "train_iter = iter(train_value_tuples)\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw: So obsessed is she with cinema, that she worships the popular actor Jaishankar\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m example_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSo obsessed is she with cinema, that she worships the popular actor Jaishankar\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw:\u001b[39m\u001b[38;5;124m'\u001b[39m,example_text)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned:\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43mtokenizer\u001b[49m(example_text))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbow:\u001b[39m\u001b[38;5;124m'\u001b[39m,vocab(tokenizer(example_text)))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mvocab length:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(vocab))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# # Named entities, stop words and punctuation are STILL part of the vocabulary\n",
    "\n",
    "example_text = 'So obsessed is she with cinema, that she worships the popular actor Jaishankar'\n",
    "print('raw:',example_text)\n",
    "print('cleaned:',tokenizer(example_text))\n",
    "print('bow:',vocab(tokenizer(example_text)))\n",
    "print('\\nvocab length:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "# label pipeline does not have to do anything since values are already 0 or 1\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline('here is an natarajan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pipeline(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data batch and iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer (Bag-of-Words)\n",
    "bow_vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Convert Bag-of-Words to torch tensor and train\u001b[39;00m\n\u001b[1;32m     36\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train_bow\u001b[38;5;241m.\u001b[39mtoarray(), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 37\u001b[0m y_train_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m train_model(model, X_train_tensor, y_train_tensor)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define the model, loss, and optimizer\n",
    "input_dim = X_train_bow.shape[1]  # Adjust according to your input size (5000 for Bag-of-Words/TF-IDF)\n",
    "num_classes = len(y.unique())  # Number of unique languages\n",
    "model = TextClassificationModel(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (simplified)\n",
    "def train_model(model, X_train, y_train):\n",
    "    model.train()\n",
    "    for epoch in range(5):  # Training for 5 epochs\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/5], Loss: {loss.item():.4f}')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Convert Bag-of-Words to torch tensor and train\n",
    "X_train_tensor = torch.tensor(X_train_bow.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "train_model(model, X_train_tensor, y_train_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Convert TF-IDF to torch tensor and train\u001b[39;00m\n\u001b[1;32m     33\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train_tfidf\u001b[38;5;241m.\u001b[39mtoarray(), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 34\u001b[0m y_train_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m train_model(model, X_train_tensor, y_train_tensor)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define the model, loss, and optimizer\n",
    "input_dim = X_train_bow.shape[1]  # Adjust according to your input size (5000 for Bag-of-Words/TF-IDF)\n",
    "num_classes = len(y.unique())  # Number of unique languages\n",
    "model = TextClassificationModel(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (simplified)\n",
    "def train_model(model, X_train, y_train):\n",
    "    model.train()\n",
    "    for epoch in range(5):  # Training for 5 epochs\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/5], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Convert TF-IDF to torch tensor and train\n",
    "X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "train_model(model, X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(\u001b[43mX_test_tensor\u001b[49m)\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m      9\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_tensor, y_pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Get model predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).argmax(dim=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_tensor, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_tensor, y_pred)\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
