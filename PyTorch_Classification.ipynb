{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show torch\n",
    "#!pip show torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>release_year</th>\n",
       "      <th>language</th>\n",
       "      <th>genre</th>\n",
       "      <th>overview</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>popularity</th>\n",
       "      <th>cleaned_overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pad Man</td>\n",
       "      <td>2018</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>upon realizing extent woman affected menses se...</td>\n",
       "      <td>7.420</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.036</td>\n",
       "      <td>upon realizing extent woman affected menses se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tamasha</td>\n",
       "      <td>2015</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>meeting vacation ved tara sense connection vow...</td>\n",
       "      <td>6.720</td>\n",
       "      <td>141.0</td>\n",
       "      <td>8.770</td>\n",
       "      <td>meeting vacation ved tara sense connection vow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tu Jhoothi Main Makkaar</td>\n",
       "      <td>2023</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>earn extra cash mickey help couple break life ...</td>\n",
       "      <td>6.253</td>\n",
       "      <td>144.0</td>\n",
       "      <td>10.045</td>\n",
       "      <td>earn extra cash mickey help couple break life ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi Medium</td>\n",
       "      <td>2017</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>mita raj batra affluent couple delhi chandni c...</td>\n",
       "      <td>7.300</td>\n",
       "      <td>166.0</td>\n",
       "      <td>7.001</td>\n",
       "      <td>mita raj batra affluent couple delhi chandni c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dilwale</td>\n",
       "      <td>2015</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>raj mafia member one day meet girl meera chasi...</td>\n",
       "      <td>6.648</td>\n",
       "      <td>301.0</td>\n",
       "      <td>11.501</td>\n",
       "      <td>raj mafia member one day meet girl meera chasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title  release_year language  genre  \\\n",
       "0                  Pad Man          2018  Indisch  Humor   \n",
       "1                  Tamasha          2015  Indisch  Humor   \n",
       "2  Tu Jhoothi Main Makkaar          2023  Indisch  Humor   \n",
       "3             Hindi Medium          2017  Indisch  Humor   \n",
       "4                  Dilwale          2015  Indisch  Humor   \n",
       "\n",
       "                                            overview  vote_average  \\\n",
       "0  upon realizing extent woman affected menses se...         7.420   \n",
       "1  meeting vacation ved tara sense connection vow...         6.720   \n",
       "2  earn extra cash mickey help couple break life ...         6.253   \n",
       "3  mita raj batra affluent couple delhi chandni c...         7.300   \n",
       "4  raj mafia member one day meet girl meera chasi...         6.648   \n",
       "\n",
       "   vote_count  popularity                                   cleaned_overview  \n",
       "0       200.0       7.036  upon realizing extent woman affected menses se...  \n",
       "1       141.0       8.770  meeting vacation ved tara sense connection vow...  \n",
       "2       144.0      10.045  earn extra cash mickey help couple break life ...  \n",
       "3       166.0       7.001  mita raj batra affluent couple delhi chandni c...  \n",
       "4       301.0      11.501  raj mafia member one day meet girl meera chasi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/movies_2015_2023_preprocessed_genre.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>release_year</th>\n",
       "      <th>language</th>\n",
       "      <th>genre</th>\n",
       "      <th>overview</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>popularity</th>\n",
       "      <th>cleaned_overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pad Man</td>\n",
       "      <td>2018</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>upon realizing extent woman affected menses se...</td>\n",
       "      <td>7.420</td>\n",
       "      <td>200.0</td>\n",
       "      <td>7.036</td>\n",
       "      <td>upon realizing extent woman affected menses se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tamasha</td>\n",
       "      <td>2015</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>meeting vacation ved tara sense connection vow...</td>\n",
       "      <td>6.720</td>\n",
       "      <td>141.0</td>\n",
       "      <td>8.770</td>\n",
       "      <td>meeting vacation ved tara sense connection vow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tu Jhoothi Main Makkaar</td>\n",
       "      <td>2023</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>earn extra cash mickey help couple break life ...</td>\n",
       "      <td>6.253</td>\n",
       "      <td>144.0</td>\n",
       "      <td>10.045</td>\n",
       "      <td>earn extra cash mickey help couple break life ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi Medium</td>\n",
       "      <td>2017</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>mita raj batra affluent couple delhi chandni c...</td>\n",
       "      <td>7.300</td>\n",
       "      <td>166.0</td>\n",
       "      <td>7.001</td>\n",
       "      <td>mita raj batra affluent couple delhi chandni c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dilwale</td>\n",
       "      <td>2015</td>\n",
       "      <td>Indisch</td>\n",
       "      <td>Humor</td>\n",
       "      <td>raj mafia member one day meet girl meera chasi...</td>\n",
       "      <td>6.648</td>\n",
       "      <td>301.0</td>\n",
       "      <td>11.501</td>\n",
       "      <td>raj mafia member one day meet girl meera chasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title  release_year language  genre  \\\n",
       "0                  Pad Man          2018  Indisch  Humor   \n",
       "1                  Tamasha          2015  Indisch  Humor   \n",
       "2  Tu Jhoothi Main Makkaar          2023  Indisch  Humor   \n",
       "3             Hindi Medium          2017  Indisch  Humor   \n",
       "4                  Dilwale          2015  Indisch  Humor   \n",
       "\n",
       "                                            overview  vote_average  \\\n",
       "0  upon realizing extent woman affected menses se...         7.420   \n",
       "1  meeting vacation ved tara sense connection vow...         6.720   \n",
       "2  earn extra cash mickey help couple break life ...         6.253   \n",
       "3  mita raj batra affluent couple delhi chandni c...         7.300   \n",
       "4  raj mafia member one day meet girl meera chasi...         6.648   \n",
       "\n",
       "   vote_count  popularity                                   cleaned_overview  \n",
       "0       200.0       7.036  upon realizing extent woman affected menses se...  \n",
       "1       141.0       8.770  meeting vacation ved tara sense connection vow...  \n",
       "2       144.0      10.045  earn extra cash mickey help couple break life ...  \n",
       "3       166.0       7.001  mita raj batra affluent couple delhi chandni c...  \n",
       "4       301.0      11.501  raj mafia member one day meet girl meera chasi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values and remove them\n",
    "df_cleaned = df.dropna(subset=['cleaned_overview', 'language', 'genre'])\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 'cleaned_overview' and 'genre' columns properly to avoid the warning\n",
    "df_cleaned = df_cleaned.copy()\n",
    "df_cleaned.loc[:, 'combined_text'] = df_cleaned['cleaned_overview'].fillna('') + ' ' + df_cleaned['genre'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['cleaned_overview'] = df_cleaned['cleaned_overview'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df_cleaned['combined_text']\n",
    "y = df_cleaned['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Convert the 'language' column to numeric labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'label' to store the encoded language labels\n",
    "df_cleaned['label'] = label_encoder.fit_transform(df_cleaned['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of value tuples: 15499\n",
      "First tuple: (3, 'upon realizing extent woman affected menses set create sanitary pad machine provide inexpensive sanitary pad woman rural india')\n",
      "Last tuple: (4, 'several european underground director turn ancient rule mankind inside shocking goresoaked interpretation god law bible world ending commandment dying')\n"
     ]
    }
   ],
   "source": [
    "# Create value tuples (label, cleaned_overview)\n",
    "value_tuples = []\n",
    "for _, row in df_cleaned.iterrows():\n",
    "    value_tuples.append((row['label'], row['cleaned_overview']))\n",
    "\n",
    "# Print the length and a few examples\n",
    "print(f\"Total number of value tuples: {len(value_tuples)}\")\n",
    "print(\"First tuple:\", value_tuples[0])\n",
    "print(\"Last tuple:\", value_tuples[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using a scikit-learn-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### building a custom PyTorch classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 80% training and 20% testing\n",
    "#train_value_tuples, test_value_tuples = train_test_split(value_tuples, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the size of the training and testing sets\n",
    "#print(f\"Training set size: {len(train_value_tuples)}\")\n",
    "#print(f\"Testing set size: {len(test_value_tuples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=30000)  # Limiting to top 5000 features we can use 30000 like knn to see the difference\n",
    "\n",
    "#custom PyTorch\n",
    "\n",
    "#X_train_tfidf = vectorizer.fit_transform([text for label, text in train_value_tuples])\n",
    "#X_test_tfidf = vectorizer.transform([text for label, text in test_value_tuples])\n",
    "\n",
    "# using a scikit-learn-based model\n",
    "\n",
    "# Fit the vectorizer on the training data and transform it\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the fitted vectorizer\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Converting TF-IDF matrices to dense tensors\n",
    "X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "\n",
    "# Convert y_train and y_test to tensors\n",
    "y_train_tensor = torch.tensor(label_encoder.transform(y_train.values), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(label_encoder.transform(y_test.values), dtype=torch.long)\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = MovieDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = MovieDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader objects\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the PyTorch Model\n",
    "define a simple feed-forward neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "#class TextClassificationModel(nn.Module):\n",
    "#    def __init__(self, input_size, num_classes):\n",
    "#        super(TextClassificationModel, self).__init__()\n",
    "#        self.fc1 = nn.Linear(input_size, 128)\n",
    "#        self.fc2 = nn.Linear(128, 64)\n",
    "#        self.fc3 = nn.Linear(64, num_classes)\n",
    "#\n",
    "#    def forward(self, x):\n",
    "#        x = F.relu(self.fc1(x))\n",
    "#        x = F.relu(self.fc2(x))\n",
    "#        x = self.fc3(x)\n",
    "#        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce Model Complexity\n",
    "We will start by simplifying the model. A smaller network reduces the risk of overfitting, particularly when the dataset size is relatively small for a complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # Reduced the number of neurons\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (fc1): Linear(in_features=28191, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "input_size = X_train_tfidf.shape[1]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = TextClassificationModel(input_size, num_classes)\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Loss Function and Optimizer\n",
    "We’ll use cross-entropy loss for this multi-class classification problem and an optimizer like Adam for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Adding weight decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "we feed batches of data to the model and optimize it using backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.6445, Accuracy: 0.3147\n",
      "Epoch [2/10], Loss: 1.0544, Accuracy: 0.6284\n",
      "Epoch [3/10], Loss: 0.4400, Accuracy: 0.8906\n",
      "Epoch [4/10], Loss: 0.1525, Accuracy: 0.9717\n",
      "Epoch [5/10], Loss: 0.0562, Accuracy: 0.9937\n",
      "Epoch [6/10], Loss: 0.0239, Accuracy: 0.9986\n",
      "Epoch [7/10], Loss: 0.0124, Accuracy: 0.9996\n",
      "Epoch [8/10], Loss: 0.0076, Accuracy: 0.9998\n",
      "Epoch [9/10], Loss: 0.0050, Accuracy: 0.9998\n",
      "Epoch [10/10], Loss: 0.0036, Accuracy: 0.9998\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "After training, we can evaluate our model’s performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7129\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## without Reduce Model Complexity Test Accuracy: 0.7232\n",
    "## with Reduce Model Complexity and L2 Regularization (Weight Decay) Test Accuracy: 0.6900\n",
    "## with  Reduce Model Complexity Test Accuracy: 0.7300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch size: 32\n",
      "Epoch [1/10], Loss: 1.5310, Accuracy: 0.3797\n",
      "Epoch [2/10], Loss: 0.8077, Accuracy: 0.7384\n",
      "Epoch [3/10], Loss: 0.3014, Accuracy: 0.9237\n",
      "Epoch [4/10], Loss: 0.0952, Accuracy: 0.9823\n",
      "Epoch [5/10], Loss: 0.0302, Accuracy: 0.9974\n",
      "Epoch [6/10], Loss: 0.0116, Accuracy: 0.9995\n",
      "Epoch [7/10], Loss: 0.0061, Accuracy: 0.9997\n",
      "Epoch [8/10], Loss: 0.0036, Accuracy: 0.9998\n",
      "Epoch [9/10], Loss: 0.0027, Accuracy: 0.9998\n",
      "Epoch [10/10], Loss: 0.0018, Accuracy: 0.9999\n",
      "Test Accuracy: 0.6977\n",
      "Training with batch size: 64\n",
      "Epoch [1/10], Loss: 1.6111, Accuracy: 0.3293\n",
      "Epoch [2/10], Loss: 0.8806, Accuracy: 0.6930\n",
      "Epoch [3/10], Loss: 0.3889, Accuracy: 0.9006\n",
      "Epoch [4/10], Loss: 0.1607, Accuracy: 0.9698\n",
      "Epoch [5/10], Loss: 0.0641, Accuracy: 0.9922\n",
      "Epoch [6/10], Loss: 0.0276, Accuracy: 0.9983\n",
      "Epoch [7/10], Loss: 0.0140, Accuracy: 0.9995\n",
      "Epoch [8/10], Loss: 0.0085, Accuracy: 0.9997\n",
      "Epoch [9/10], Loss: 0.0057, Accuracy: 0.9998\n",
      "Epoch [10/10], Loss: 0.0042, Accuracy: 0.9998\n",
      "Test Accuracy: 0.6997\n",
      "Training with batch size: 128\n",
      "Epoch [1/10], Loss: 1.7051, Accuracy: 0.2797\n",
      "Epoch [2/10], Loss: 1.2570, Accuracy: 0.5882\n",
      "Epoch [3/10], Loss: 0.6739, Accuracy: 0.8135\n",
      "Epoch [4/10], Loss: 0.3300, Accuracy: 0.9306\n",
      "Epoch [5/10], Loss: 0.1585, Accuracy: 0.9791\n",
      "Epoch [6/10], Loss: 0.0758, Accuracy: 0.9927\n",
      "Epoch [7/10], Loss: 0.0394, Accuracy: 0.9972\n",
      "Epoch [8/10], Loss: 0.0228, Accuracy: 0.9993\n",
      "Epoch [9/10], Loss: 0.0147, Accuracy: 0.9996\n",
      "Epoch [10/10], Loss: 0.0103, Accuracy: 0.9997\n",
      "Test Accuracy: 0.7129\n",
      "Training with batch size: 256\n",
      "Epoch [1/10], Loss: 1.7554, Accuracy: 0.2221\n",
      "Epoch [2/10], Loss: 1.5703, Accuracy: 0.3225\n",
      "Epoch [3/10], Loss: 1.2252, Accuracy: 0.5984\n",
      "Epoch [4/10], Loss: 0.8377, Accuracy: 0.7885\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Train the model for 10 epochs with the current batch size\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     25\u001b[0m evaluate_model(model, test_loader)\n",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mTextClassificationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# List of batch sizes to try\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "\n",
    "# Loop over each batch size\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Training with batch size: {batch_size}\")\n",
    "    \n",
    "    # Create new DataLoader with the current batch size\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Reinitialize the model\n",
    "    model = TextClassificationModel(input_size, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Reinitialize the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model for 10 epochs with the current batch size\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with weight decay: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.7589, Accuracy: 0.2510\n",
      "Epoch [2/10], Loss: 1.5761, Accuracy: 0.4048\n",
      "Epoch [3/10], Loss: 1.1870, Accuracy: 0.6119\n",
      "Epoch [4/10], Loss: 0.7679, Accuracy: 0.7751\n",
      "Epoch [5/10], Loss: 0.4931, Accuracy: 0.8498\n",
      "Epoch [6/10], Loss: 0.3107, Accuracy: 0.9343\n",
      "Epoch [7/10], Loss: 0.1852, Accuracy: 0.9749\n",
      "Epoch [8/10], Loss: 0.1095, Accuracy: 0.9891\n",
      "Epoch [9/10], Loss: 0.0681, Accuracy: 0.9948\n",
      "Epoch [10/10], Loss: 0.0443, Accuracy: 0.9981\n",
      "Test Accuracy: 0.7187\n",
      "Training with weight decay: 0.0001\n",
      "Epoch [1/10], Loss: 1.7630, Accuracy: 0.2534\n",
      "Epoch [2/10], Loss: 1.6261, Accuracy: 0.4382\n",
      "Epoch [3/10], Loss: 1.3084, Accuracy: 0.5308\n",
      "Epoch [4/10], Loss: 0.9315, Accuracy: 0.6915\n",
      "Epoch [5/10], Loss: 0.6084, Accuracy: 0.8312\n",
      "Epoch [6/10], Loss: 0.3856, Accuracy: 0.9295\n",
      "Epoch [7/10], Loss: 0.2336, Accuracy: 0.9683\n",
      "Epoch [8/10], Loss: 0.1455, Accuracy: 0.9848\n",
      "Epoch [9/10], Loss: 0.0943, Accuracy: 0.9929\n",
      "Epoch [10/10], Loss: 0.0650, Accuracy: 0.9975\n",
      "Test Accuracy: 0.7258\n",
      "Training with weight decay: 0.001\n",
      "Epoch [1/10], Loss: 1.7563, Accuracy: 0.2530\n",
      "Epoch [2/10], Loss: 1.6650, Accuracy: 0.2967\n",
      "Epoch [3/10], Loss: 1.5120, Accuracy: 0.3945\n",
      "Epoch [4/10], Loss: 1.3517, Accuracy: 0.4389\n",
      "Epoch [5/10], Loss: 1.1920, Accuracy: 0.5338\n",
      "Epoch [6/10], Loss: 1.0026, Accuracy: 0.6963\n",
      "Epoch [7/10], Loss: 0.8195, Accuracy: 0.7672\n",
      "Epoch [8/10], Loss: 0.6651, Accuracy: 0.8114\n",
      "Epoch [9/10], Loss: 0.5459, Accuracy: 0.8454\n",
      "Epoch [10/10], Loss: 0.4515, Accuracy: 0.8747\n",
      "Test Accuracy: 0.6597\n",
      "Training with weight decay: 0.01\n",
      "Epoch [1/10], Loss: 1.7765, Accuracy: 0.2069\n",
      "Epoch [2/10], Loss: 1.7223, Accuracy: 0.2883\n",
      "Epoch [3/10], Loss: 1.7140, Accuracy: 0.2883\n",
      "Epoch [4/10], Loss: 1.7150, Accuracy: 0.2883\n",
      "Epoch [5/10], Loss: 1.7155, Accuracy: 0.2883\n",
      "Epoch [6/10], Loss: 1.7155, Accuracy: 0.2883\n",
      "Epoch [7/10], Loss: 1.7147, Accuracy: 0.2883\n",
      "Epoch [8/10], Loss: 1.7149, Accuracy: 0.2883\n",
      "Epoch [9/10], Loss: 1.7144, Accuracy: 0.2883\n",
      "Epoch [10/10], Loss: 1.7148, Accuracy: 0.2883\n",
      "Test Accuracy: 0.2919\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define different weight decay values to experiment with\n",
    "weight_decay_values = [0, 1e-4, 1e-3, 1e-2]\n",
    "\n",
    "# Loop over the different weight decay values\n",
    "for weight_decay in weight_decay_values:\n",
    "    print(f\"Training with weight decay: {weight_decay}\")\n",
    "    \n",
    "    # Reinitialize the model\n",
    "    model = TextClassificationModel(input_size, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Reinitialize the optimizer with the current weight decay\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "\n",
    "    # Train the model for 10 epochs with the current weight decay\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate: 0.0001\n",
      "Epoch [1/10], Loss: 1.7870, Accuracy: 0.2071\n",
      "Epoch [2/10], Loss: 1.7790, Accuracy: 0.2071\n",
      "Epoch [3/10], Loss: 1.7680, Accuracy: 0.2071\n",
      "Epoch [4/10], Loss: 1.7534, Accuracy: 0.2822\n",
      "Epoch [5/10], Loss: 1.7353, Accuracy: 0.4414\n",
      "Epoch [6/10], Loss: 1.7118, Accuracy: 0.3534\n",
      "Epoch [7/10], Loss: 1.6853, Accuracy: 0.3132\n",
      "Epoch [8/10], Loss: 1.6548, Accuracy: 0.3032\n",
      "Epoch [9/10], Loss: 1.6214, Accuracy: 0.3085\n",
      "Epoch [10/10], Loss: 1.5851, Accuracy: 0.3189\n",
      "Test Accuracy: 0.3242\n",
      "Training with learning rate: 0.001\n",
      "Epoch [1/10], Loss: 1.8130, Accuracy: 0.1712\n",
      "Epoch [2/10], Loss: 1.6552, Accuracy: 0.3138\n",
      "Epoch [3/10], Loss: 1.3906, Accuracy: 0.4531\n",
      "Epoch [4/10], Loss: 1.0261, Accuracy: 0.6544\n",
      "Epoch [5/10], Loss: 0.6626, Accuracy: 0.8012\n",
      "Epoch [6/10], Loss: 0.3842, Accuracy: 0.9308\n",
      "Epoch [7/10], Loss: 0.2089, Accuracy: 0.9705\n",
      "Epoch [8/10], Loss: 0.1186, Accuracy: 0.9866\n",
      "Epoch [9/10], Loss: 0.0710, Accuracy: 0.9939\n",
      "Epoch [10/10], Loss: 0.0454, Accuracy: 0.9981\n",
      "Test Accuracy: 0.7152\n",
      "Training with learning rate: 0.01\n",
      "Epoch [1/10], Loss: 1.4566, Accuracy: 0.4037\n",
      "Epoch [2/10], Loss: 0.4737, Accuracy: 0.8539\n",
      "Epoch [3/10], Loss: 0.0668, Accuracy: 0.9826\n",
      "Epoch [4/10], Loss: 0.0092, Accuracy: 0.9984\n",
      "Epoch [5/10], Loss: 0.0023, Accuracy: 0.9998\n",
      "Epoch [6/10], Loss: 0.0013, Accuracy: 0.9997\n",
      "Epoch [7/10], Loss: 0.0008, Accuracy: 0.9998\n",
      "Epoch [8/10], Loss: 0.0010, Accuracy: 0.9998\n",
      "Epoch [9/10], Loss: 0.0010, Accuracy: 0.9997\n",
      "Epoch [10/10], Loss: 0.0009, Accuracy: 0.9998\n",
      "Test Accuracy: 0.7187\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-4, 1e-3, 1e-2]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with learning rate: {lr}\")\n",
    "    \n",
    "    # Reinitialize the model\n",
    "    model = TextClassificationModel(input_size, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Reinitialize the optimizer with the current learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train the model with the current learning rate\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    evaluate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
